{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12675,"status":"ok","timestamp":1712315367126,"user":{"displayName":"Fabian F","userId":"04447055572954868898"},"user_tz":-120},"id":"tIEher0OlQ61","outputId":"36ee0bda-91bf-48aa-e956-a4b9b8963371"},"outputs":[],"source":["#!pip install wandb -qU"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":12220,"status":"ok","timestamp":1712315379343,"user":{"displayName":"Fabian F","userId":"04447055572954868898"},"user_tz":-120},"id":"zJ7xNU3NcCwz"},"outputs":[],"source":["from __future__ import annotations\n","\n","from datetime import datetime, timezone\n","from pathlib import Path\n","\n","import torch\n","import torch.nn.functional as F\n","import torchvision\n","from torch import nn, optim\n","from torch.utils.data import DataLoader, Dataset, Subset\n","from torchvision import transforms\n","from tqdm import tqdm\n","\n","import wandb"]},{"cell_type":"markdown","metadata":{"id":"zk95CyQP6Sfa"},"source":["## Weights and Bias Login\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"elapsed":26879,"status":"ok","timestamp":1712315406219,"user":{"displayName":"Fabian F","userId":"04447055572954868898"},"user_tz":-120},"id":"Amah_vX1lNE-","outputId":"a5248a1e-9738-463e-c574-1eb65ccc9b9c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","wandb: Currently logged in as: fabianfuchs. Use `wandb login --relogin` to force relogin\n"]},{"data":{"text/plain":["True"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login()"]},{"cell_type":"markdown","metadata":{"id":"7odmCw657Me2"},"source":["## Model\n"]},{"cell_type":"markdown","metadata":{},"source":["Got source code for the ConvNeXtV2 model from https://github.com/facebookresearch/ConvNeXt-V2/blob/main/models/convnextv2.py and removed drop path and custom weight initialization.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":209,"status":"ok","timestamp":1712315409552,"user":{"displayName":"Fabian F","userId":"04447055572954868898"},"user_tz":-120},"id":"kA6kljA77MAe"},"outputs":[],"source":["class LayerNorm(nn.Module):\n","    \"\"\"LayerNorm that supports two data formats: channels_last (default) or channels_first.\n","    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n","    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n","    with shape (batch_size, channels, height, width).\n","    \"\"\"\n","\n","    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(normalized_shape))\n","        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n","        self.eps = eps\n","        self.data_format = data_format\n","        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n","            raise NotImplementedError\n","        self.normalized_shape = (normalized_shape,)\n","\n","    def forward(self, x):\n","        if self.data_format == \"channels_last\":\n","            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n","        elif self.data_format == \"channels_first\":\n","            u = x.mean(1, keepdim=True)\n","            s = (x - u).pow(2).mean(1, keepdim=True)\n","            x = (x - u) / torch.sqrt(s + self.eps)\n","            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n","            return x\n","\n","\n","class GRN(nn.Module):\n","    \"\"\"GRN (Global Response Normalization) layer\"\"\"\n","\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.gamma = nn.Parameter(torch.zeros(1, 1, 1, dim))\n","        self.beta = nn.Parameter(torch.zeros(1, 1, 1, dim))\n","\n","    def forward(self, x):\n","        Gx = torch.norm(x, p=2, dim=(1, 2), keepdim=True)\n","        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n","        return self.gamma * (x * Nx) + self.beta + x\n","\n","\n","class Block(nn.Module):\n","    \"\"\"ConvNeXtV2 Block.\"\"\"\n","\n","    def __init__(self, dim, drop_path=0.0):\n","        \"\"\"ConvNeXtV2 Block.\n","\n","        Args:\n","            dim (int): Number of input channels.\n","            drop_path (float): Stochastic depth rate. Default: 0.0\n","        \"\"\"\n","        super().__init__()\n","        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)  # depthwise conv\n","        self.norm = LayerNorm(dim, eps=1e-6)\n","        self.pwconv1 = nn.Linear(dim, 4 * dim)  # pointwise/1x1 convs, implemented with linear layers\n","        self.act = nn.GELU()\n","        self.grn = GRN(4 * dim)\n","        self.pwconv2 = nn.Linear(4 * dim, dim)\n","\n","    def forward(self, x):\n","        input = x\n","        x = self.dwconv(x)\n","        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n","        x = self.norm(x)\n","        x = self.pwconv1(x)\n","        x = self.act(x)\n","        x = self.grn(x)\n","        x = self.pwconv2(x)\n","        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n","\n","        return input + x\n","\n","\n","class ConvNeXtV2(nn.Module):\n","    \"\"\"ConvNeXt V2.\"\"\"\n","\n","    def __init__(\n","        self,\n","        in_chans=3,\n","        num_classes=1000,\n","        depths=[3, 3, 9, 3],\n","        dims=[96, 192, 384, 768],\n","        drop_path_rate=0.0,\n","    ):\n","        \"\"\"ConvNeXt V2.\n","\n","        Args:\n","            in_chans (int): Number of input image channels. Default: 3\n","            num_classes (int): Number of classes for classification head. Default: 1000\n","            depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n","            dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n","            drop_path_rate (float): Stochastic depth rate. Default: 0.\n","            head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n","        \"\"\"\n","        super().__init__()\n","        self.depths = depths\n","        self.downsample_layers = nn.ModuleList()  # stem and 3 intermediate downsampling conv layers\n","        stem = nn.Sequential(\n","            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n","            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\"),\n","        )\n","        self.downsample_layers.append(stem)\n","        for i in range(3):\n","            downsample_layer = nn.Sequential(\n","                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n","                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2),\n","            )\n","            self.downsample_layers.append(downsample_layer)\n","\n","        self.stages = nn.ModuleList()  # 4 feature resolution stages, each consisting of multiple residual blocks\n","        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n","        cur = 0\n","        for i in range(4):\n","            stage = nn.Sequential(*[Block(dim=dims[i], drop_path=dp_rates[cur + j]) for j in range(depths[i])])\n","            self.stages.append(stage)\n","            cur += depths[i]\n","\n","        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # final norm layer\n","        self.head = nn.Linear(dims[-1], num_classes)\n","\n","    def forward_features(self, x):\n","        for i in range(4):\n","            x = self.downsample_layers[i](x)\n","            x = self.stages[i](x)\n","        return self.norm(x.mean([-2, -1]))  # global average pooling, (N, C, H, W) -> (N, C)\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        x = self.head(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"gS1TSvMr6VrH"},"source":["## Functions\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":215,"status":"ok","timestamp":1712315412651,"user":{"displayName":"Fabian F","userId":"04447055572954868898"},"user_tz":-120},"id":"QNKuM6tj6Q7q"},"outputs":[],"source":["def get_classes() -> tuple:\n","    \"\"\"Return class labels of CIFAR-10 dataset.\"\"\"\n","    return (\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n","\n","\n","def get_datasets(tasks: int) -> tuple[list[Subset], list[Subset]]:\n","    \"\"\"Split CIFAR-10 dataset into task specific subsets.\n","\n","    Args:\n","        tasks (int): Number of tasks to split the dataset into.\n","\n","    Returns:\n","        tuple[list[Subset], list[Subset]]: Tuple containing two list with the train and test subsets.\n","    \"\"\"\n","    classes = get_classes()\n","    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","    trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n","    testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n","    classes_per_task = torch.linspace(0, len(classes), tasks + 1, dtype=torch.int)\n","    trainsets = []\n","    testsets = []\n","    train_targets = torch.tensor(trainset.targets)\n","    test_targets = torch.tensor(testset.targets)\n","    for i in range(len(classes_per_task) - 1):\n","        train_indices = []\n","        test_indices = []\n","        for j in range(classes_per_task[i], classes_per_task[i + 1]):\n","            train_indices.extend((train_targets == j).nonzero(as_tuple=False).flatten().tolist())\n","            test_indices.extend((test_targets == j).nonzero(as_tuple=False).flatten().tolist())\n","        trainsets.append(Subset(trainset, train_indices))\n","        testsets.append(Subset(testset, test_indices))\n","    return trainsets, testsets"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1712315443516,"user":{"displayName":"Fabian F","userId":"04447055572954868898"},"user_tz":-120},"id":"IbR9W9Tw7AVI"},"outputs":[],"source":["def train_standard_training_pipeline(\n","    run_name: str,\n","    trainset: Dataset,\n","    testset: Dataset,\n","    model: nn.Module,\n","    device: torch.device,\n","    criterion: nn.modules.loss._Loss,\n","    optimizer: torch.optim.Optimizer,\n","    scheduler: torch.optim.lr_scheduler.LRScheduler,\n","    epochs: int,\n","    batch_size: int,\n","):\n","    # create dataloaders\n","    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n","    # move model to device\n","    model.to(device=device)\n","\n","    # setup logging\n","    project_name = \"continual_learning\"\n","    run_name = f\"{run_name}_{datetime.now(tz=timezone.utc).strftime('%Y_%m_%d_%H_%M_%S')}\"\n","    wandb.init(\n","        # Set the project where this run will be logged\n","        project=project_name,\n","        # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n","        name=run_name,\n","        # Track hyperparameters and run metadata\n","        config={\n","            \"architecture\": \"CNN\",\n","            \"dataset\": \"CIFAR-10\",\n","            \"epochs\": epochs,\n","            \"batch_size\": batch_size,\n","        },\n","    )\n","\n","    # training\n","    for epoch in range(epochs):\n","        # train one epoch\n","        with tqdm(total=len(trainloader) * batch_size, unit=\"images\") as progress_bar:\n","            model.train()\n","            for i, (images, labels) in enumerate(trainloader):\n","                progress_bar.set_description(f\"Epoch {epoch+1} Batch {i}\")\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                prediction = model(images.to(device=device))\n","                # calc loss\n","                loss = criterion(prediction, labels.to(device=device))\n","                # backward\n","                loss.backward()\n","                # optimizer\n","                optimizer.step()\n","                # scheduler\n","                scheduler.step()\n","                progress_bar.set_postfix(loss=loss.item())\n","                progress_bar.update(batch_size)\n","                wandb.log({\"loss\": loss})\n","\n","        # validate\n","        correct = 0\n","        total = 0\n","        # since we're not training, we don't need to calculate the gradients for our outputs\n","        model.eval()\n","        with tqdm(total=len(testloader) * batch_size, unit=\"images\") as progress_bar, torch.no_grad():\n","            for i, (images, labels) in enumerate(testloader):\n","                progress_bar.set_description(f\"Batch {i}\")\n","                # calculate outputs by running images through the network\n","                outputs = model(images.to(device=device))\n","                # the class with the highest energy is what we choose as prediction\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels.to(device=device)).sum().item()\n","                progress_bar.update(batch_size)\n","        accuracy = 100 * correct // total\n","        wandb.log({\"accuracy\": accuracy})\n","\n","        # save model\n","        path = Path(wandb.run.dir).joinpath(f\"model{epoch}.pth\")\n","        torch.save(model.state_dict(), path)\n","\n","    # save final model\n","    path = Path(wandb.run.dir).joinpath(\"model.pth\")\n","    torch.save(model.state_dict(), path)\n","\n","    # finish logging run\n","    wandb.finish()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def train_tasks_sequentially(  # noqa: PLR0913\n","    model: nn.Module,\n","    device: torch.device,\n","    criterion: nn.modules.loss._Loss,\n","    optimizer: torch.optim.Optimizer,\n","    scheduler: torch.optim.lr_scheduler.LRScheduler,\n","    epochs: int,\n","    batch_size: int,\n","    tasks: int,\n",") -> None:\n","    # get dataset\n","    trainsets, testsets = get_datasets(tasks=tasks)\n","\n","    for i in range(tasks):\n","        train_standard_training_pipeline(\n","            run_name=f\"task{i+1}_of_{tasks}\",\n","            trainset=trainsets[i],\n","            testset=testsets[i],\n","            model=model,\n","            device=device,\n","            criterion=criterion,\n","            optimizer=optimizer,\n","            scheduler=scheduler,\n","            epochs=epochs,\n","            batch_size=batch_size,\n","        )"]},{"cell_type":"markdown","metadata":{},"source":["broken -> seems to return same model even with different run names\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def load_trained_model(run_name: str):\n","    convnext = ConvNeXtV2(in_chans=3, num_classes=10, depths=[2, 2, 6, 2], dims=[40, 80, 160, 320])\n","\n","    best_model = wandb.restore(\n","        \"model.pth\",\n","        run_path=f\"fabianfuchs/continual_learning/{run_name}\",\n","        root=Path.cwd().joinpath(\"checkpoints\"),\n","        replace=True,\n","    )\n","\n","    # use the \"name\" attribute of the returned object if your framework expects a filename, e.g. as in Keras\n","    convnext.load_state_dict(torch.load(best_model.name))\n","    return convnext"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def accuracy(testset: Dataset, model: nn.Module, device: torch.device) -> float:\n","    testloader = DataLoader(testset, shuffle=False)\n","\n","    model.eval()\n","    correct = 0\n","    for images, labels in testloader:\n","        # calculate outputs by running images through the network\n","        predictions = model(images.to(device=device))\n","        # the class with the highest energy is what we choose as prediction\n","        _, predicted = torch.max(predictions.data, 1)\n","        correct += (predicted == labels.to(device=device)).sum().item()\n","    return 100 * correct / len(testloader)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def average_accuracy(testsets: list[Dataset], model: nn.Module, device: torch.device) -> float:\n","    average_accuracy = 0\n","    for i in range(len(testsets)):\n","        task_accuracy = accuracy(testset=testsets[i], model=model, device=device)\n","        average_accuracy += task_accuracy\n","        print(f\"{task_accuracy=}\")\n","    return average_accuracy / len(testsets)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","task_accuracy=0.0\n","task_accuracy=0.0\n","task_accuracy=0.0\n","task_accuracy=0.0\n","task_accuracy=81.3\n"]},{"data":{"text/plain":["16.259999999999998"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["trainsets, testsets = get_datasets(tasks=5)\n","model_after_task5 = load_trained_model(\"nod8s4pu\")\n","average_accuracy(testsets=testsets, model=model_after_task5, device=\"cpu\")\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","task_accuracy=0.1\n","task_accuracy=0.0\n","task_accuracy=0.0\n","task_accuracy=86.95\n","task_accuracy=0.0\n"]},{"data":{"text/plain":["17.41"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["trainsets, testsets = get_datasets(tasks=5)\n","model_after_task4 = load_trained_model(\"ips9plod\")\n","average_accuracy(testsets=testsets, model=model_after_task4, device=\"cpu\")\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","task_accuracy=0.0\n","task_accuracy=0.0\n","task_accuracy=76.65\n","task_accuracy=0.0\n","task_accuracy=0.0\n"]},{"data":{"text/plain":["15.330000000000002"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["trainsets, testsets = get_datasets(tasks=5)\n","model_after_task3 = load_trained_model(\"90uuu6hf\")\n","average_accuracy(testsets=testsets, model=model_after_task3, device=\"cpu\")\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","task_accuracy=0.0\n","task_accuracy=75.4\n","task_accuracy=0.0\n","task_accuracy=0.0\n","task_accuracy=0.0\n"]},{"data":{"text/plain":["15.080000000000002"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["trainsets, testsets = get_datasets(tasks=5)\n","model_after_task2 = load_trained_model(\"la3fepkv\")\n","average_accuracy(testsets=testsets, model=model_after_task2, device=\"cpu\")\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","task_accuracy=85.9\n","task_accuracy=0.0\n","task_accuracy=0.0\n","task_accuracy=0.0\n","task_accuracy=0.0\n"]},{"data":{"text/plain":["17.18"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["trainsets, testsets = get_datasets(tasks=5)\n","model_after_task1 = load_trained_model(\"l7eev0gl\")\n","average_accuracy(testsets=testsets, model=model_after_task1, device=\"cpu\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Standard Setting\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["epochs = 100\n","batch_size = 64\n","lr = 0.001\n","momentum = 0.9\n","device = torch.device(\"cpu\")\n","tasks = 1"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1712301521169,"user":{"displayName":"Fabian F","userId":"04447055572954868898"},"user_tz":-120},"id":"f_2dctx9ckir"},"outputs":[],"source":["convnext = ConvNeXtV2(in_chans=3, num_classes=10, depths=[2, 2, 6, 2], dims=[40, 80, 160, 320])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optimizer = optim.SGD(convnext.parameters(), lr=lr, momentum=momentum)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scheduler = torch.optim.lr_scheduler.OneCycleLR(\n","    optimizer,\n","    max_lr=lr,\n","    steps_per_epoch=5000 // tasks,\n","    epochs=epochs,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1712302779941,"user":{"displayName":"Fabian F","userId":"04447055572954868898"},"user_tz":-120},"id":"0JqT_GCKNG3T"},"outputs":[],"source":["train_tasks_sequentially(\n","    model=convnext,\n","    device=device,\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    criterion=criterion,\n","    tasks=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# train_tasks_sequentially(\n","#     model=convnext,\n","#     device=torch.device(\"cpu\"),\n","#     optimizer=optimizer,\n","#     scheduler=scheduler,\n","#     criterion=criterion,\n","#     tasks=5,\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOQvTgp7nDoJbAvqKe/0cP4","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
