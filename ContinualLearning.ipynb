{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIEher0OlQ61",
        "outputId": "646021aa-964a-4b61-d5ee-5345a2995b79"
      },
      "outputs": [],
      "source": [
        "#!pip install wandb -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "zJ7xNU3NcCwz",
        "outputId": "b33e221c-3287-4505-8379-f35213824cf1"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from copy import deepcopy\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import ConcatDataset, DataLoader, Dataset, Subset\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk95CyQP6Sfa"
      },
      "source": [
        "## Weights and Bias Login\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Amah_vX1lNE-"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7odmCw657Me2"
      },
      "source": [
        "## Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ConvNeXtV2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL0UcU1cGeqP"
      },
      "source": [
        "Got source code for the ConvNeXtV2 model from https://github.com/facebookresearch/ConvNeXt-V2/blob/main/models/convnextv2.py and removed drop path and custom weight initialization. Added variable patch size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA6kljA77MAe"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"LayerNorm that supports two data formats: channels_last (default) or channels_first.\n",
        "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n",
        "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n",
        "    with shape (batch_size, channels, height, width).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.eps = eps\n",
        "        self.data_format = data_format\n",
        "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
        "            raise NotImplementedError\n",
        "        self.normalized_shape = (normalized_shape,)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.data_format == \"channels_last\":\n",
        "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        elif self.data_format == \"channels_first\":\n",
        "            u = x.mean(1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.eps)\n",
        "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
        "            return x\n",
        "\n",
        "\n",
        "class GRN(nn.Module):\n",
        "    \"\"\"GRN (Global Response Normalization) layer\"\"\"\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.zeros(1, 1, 1, dim))\n",
        "        self.beta = nn.Parameter(torch.zeros(1, 1, 1, dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        Gx = torch.norm(x, p=2, dim=(1, 2), keepdim=True)\n",
        "        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n",
        "        return self.gamma * (x * Nx) + self.beta + x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"ConvNeXtV2 Block.\"\"\"\n",
        "\n",
        "    def __init__(self, dim, drop_path=0.0):\n",
        "        \"\"\"ConvNeXtV2 Block.\n",
        "\n",
        "        Args:\n",
        "            dim (int): Number of input channels.\n",
        "            drop_path (float): Stochastic depth rate. Default: 0.0\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)  # depthwise conv\n",
        "        self.norm = LayerNorm(dim, eps=1e-6)\n",
        "        self.pwconv1 = nn.Linear(dim, 4 * dim)  # pointwise/1x1 convs, implemented with linear layers\n",
        "        self.act = nn.GELU()\n",
        "        self.grn = GRN(4 * dim)\n",
        "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.dwconv(x)\n",
        "        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n",
        "        x = self.norm(x)\n",
        "        x = self.pwconv1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.grn(x)\n",
        "        x = self.pwconv2(x)\n",
        "        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
        "\n",
        "        return input + x\n",
        "\n",
        "\n",
        "class ConvNeXtV2(nn.Module):\n",
        "    \"\"\"ConvNeXt V2.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_chans=3,\n",
        "        num_classes=1000,\n",
        "        depths=[3, 3, 9, 3],\n",
        "        dims=[96, 192, 384, 768],\n",
        "        drop_path_rate=0.0,\n",
        "        patch_size=1,\n",
        "    ):\n",
        "        \"\"\"ConvNeXt V2.\n",
        "\n",
        "        Args:\n",
        "            in_chans (int): Number of input image channels. Default: 3\n",
        "            num_classes (int): Number of classes for classification head. Default: 1000\n",
        "            depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
        "            dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
        "            drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
        "            head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.depths = depths\n",
        "        self.downsample_layers = nn.ModuleList()  # stem and 3 intermediate downsampling conv layers\n",
        "        stem = nn.Sequential(\n",
        "            nn.Conv2d(in_chans, dims[0], kernel_size=patch_size, stride=patch_size),\n",
        "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\"),\n",
        "        )\n",
        "        self.downsample_layers.append(stem)\n",
        "        for i in range(3):\n",
        "            downsample_layer = nn.Sequential(\n",
        "                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
        "                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2),\n",
        "            )\n",
        "            self.downsample_layers.append(downsample_layer)\n",
        "\n",
        "        self.stages = nn.ModuleList()  # 4 feature resolution stages, each consisting of multiple residual blocks\n",
        "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "        cur = 0\n",
        "        for i in range(4):\n",
        "            stage = nn.Sequential(*[Block(dim=dims[i], drop_path=dp_rates[cur + j]) for j in range(depths[i])])\n",
        "            self.stages.append(stage)\n",
        "            cur += depths[i]\n",
        "\n",
        "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # final norm layer\n",
        "        self.head = nn.Linear(dims[-1], num_classes)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        for i in range(4):\n",
        "            x = self.downsample_layers[i](x)\n",
        "            x = self.stages[i](x)\n",
        "        return self.norm(x.mean([-2, -1]))  # global average pooling, (N, C, H, W) -> (N, C)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ConvMixer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "source code from here https://github.com/kentaroy47/vision-transformers-cifar10/blob/main/README.md\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fn(x) + x\n",
        "\n",
        "\n",
        "def ConvMixer(dim, depth, kernel_size=9, patch_size=7, n_classes=1000):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size),\n",
        "        nn.GELU(),\n",
        "        nn.BatchNorm2d(dim),\n",
        "        *[\n",
        "            nn.Sequential(\n",
        "                Residual(\n",
        "                    nn.Sequential(\n",
        "                        nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=\"same\"), nn.GELU(), nn.BatchNorm2d(dim)\n",
        "                    )\n",
        "                ),\n",
        "                nn.Conv2d(dim, dim, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.BatchNorm2d(dim),\n",
        "            )\n",
        "            for i in range(depth)\n",
        "        ],\n",
        "        nn.AdaptiveAvgPool2d((1, 1)),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(dim, n_classes),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS1TSvMr6VrH"
      },
      "source": [
        "## Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNKuM6tj6Q7q"
      },
      "outputs": [],
      "source": [
        "def get_classes() -> tuple:\n",
        "    \"\"\"Return class labels of CIFAR-10 dataset.\"\"\"\n",
        "    return (\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n",
        "\n",
        "\n",
        "def get_datasets(tasks: int) -> tuple[list[Subset], list[Subset]]:\n",
        "    \"\"\"Split CIFAR-10 dataset into task specific subsets.\n",
        "\n",
        "    Args:\n",
        "        tasks (int): Number of tasks to split the dataset into.\n",
        "\n",
        "    Returns:\n",
        "        tuple[list[Subset], list[Subset]]: Tuple containing two list with the train and test subsets.\n",
        "    \"\"\"\n",
        "    classes = get_classes()\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "    testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "    classes_per_task = torch.linspace(0, len(classes), tasks + 1, dtype=torch.int)\n",
        "    trainsets = []\n",
        "    testsets = []\n",
        "    train_targets = torch.tensor(trainset.targets)\n",
        "    test_targets = torch.tensor(testset.targets)\n",
        "    for i in range(len(classes_per_task) - 1):\n",
        "        train_indices = []\n",
        "        test_indices = []\n",
        "        for j in range(classes_per_task[i], classes_per_task[i + 1]):\n",
        "            train_indices.extend((train_targets == j).nonzero(as_tuple=False).flatten().tolist())\n",
        "            test_indices.extend((test_targets == j).nonzero(as_tuple=False).flatten().tolist())\n",
        "        trainsets.append(Subset(trainset, train_indices))\n",
        "        testsets.append(Subset(testset, test_indices))\n",
        "    return trainsets, testsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E3w1c8iGeqQ"
      },
      "outputs": [],
      "source": [
        "def accuracy(testset: Dataset, model: nn.Module, device: torch.device, batch_size=1) -> float:\n",
        "    testloader = DataLoader(testset, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for images, labels in testloader:\n",
        "        # calculate outputs by running images through the network\n",
        "        predictions = model(images.to(device=device))\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(predictions.data, 1)\n",
        "        correct += (predicted == labels.to(device=device)).sum().item()\n",
        "    return 100 * correct / len(testset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDcsV9XUGeqR"
      },
      "outputs": [],
      "source": [
        "def average_accuracy(\n",
        "    testsets: list[Dataset],\n",
        "    model: nn.Module,\n",
        "    device: torch.device,\n",
        "    return_intermediate: bool = False,  # noqa: FBT001, FBT002\n",
        ") -> float | tuple[float, list[float]]:\n",
        "    average_accuracy = 0\n",
        "    average_accuracies = []\n",
        "    for i in range(len(testsets)):\n",
        "        task_accuracy = accuracy(testset=testsets[i], model=model, device=device)\n",
        "        average_accuracy += task_accuracy\n",
        "        average_accuracies.append(task_accuracy)\n",
        "    if return_intermediate:\n",
        "        return average_accuracy / len(testsets), average_accuracies\n",
        "    return average_accuracy / len(testsets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def forgetting_measure(average_accuracies_per_training_per_task: list[list[float]], current_task: int) -> float:\n",
        "    forgetting_measure = 0\n",
        "    for j in range(current_task):  # exclude current task\n",
        "        f = 0\n",
        "        for i in range(j, current_task):  # exclude current task\n",
        "            f_ = (\n",
        "                average_accuracies_per_training_per_task[i][j]\n",
        "                - average_accuracies_per_training_per_task[current_task][j]\n",
        "            )\n",
        "            if f_ > f:\n",
        "                f = f_\n",
        "        forgetting_measure += f\n",
        "    return forgetting_measure / current_task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "average_accuracies_per_training_per_task = [[100], [50, 100], [25, 50, 100], [25, 25, 50, 100]]\n",
        "forgetting_measure(average_accuracies_per_training_per_task=average_accuracies_per_training_per_task, current_task=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### train loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbR9W9Tw7AVI"
      },
      "outputs": [],
      "source": [
        "def train_on_task(\n",
        "    trainset: Dataset,\n",
        "    testset: Dataset,\n",
        "    model: nn.Module,\n",
        "    device: torch.device,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    epochs: int,\n",
        "    batch_size: int,\n",
        "    lr: float,\n",
        "    criterion: nn.modules.loss._Loss | None = None,\n",
        "    scheduler: torch.optim.lr_scheduler.LRScheduler | None = None,\n",
        "):\n",
        "    # create dataloaders\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    # move model to device\n",
        "    model.to(device=device)\n",
        "\n",
        "    if criterion is None:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "    if scheduler is None:\n",
        "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer,\n",
        "            max_lr=lr,\n",
        "            steps_per_epoch=len(trainloader),\n",
        "            epochs=epochs,\n",
        "        )\n",
        "\n",
        "    # training\n",
        "    for epoch in range(epochs):\n",
        "        # train one epoch\n",
        "        with tqdm(total=len(trainset), unit=\"images\") as progress_bar:\n",
        "            model.train()\n",
        "            for i, (images, labels) in enumerate(trainloader):\n",
        "                progress_bar.set_description(f\"Epoch {epoch+1} Batch {i}\")\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                prediction = model(images.to(device=device))\n",
        "                # calc loss\n",
        "                loss = criterion(prediction, labels.to(device=device))\n",
        "                # backward\n",
        "                loss.backward()\n",
        "                # optimizer\n",
        "                optimizer.step()\n",
        "                # scheduler\n",
        "                scheduler.step()\n",
        "                progress_bar.set_postfix(loss=loss.item())\n",
        "                progress_bar.update(labels.shape[0])\n",
        "                wandb.log({\"loss\": loss})\n",
        "                wandb.log({\"lr\": scheduler.get_last_lr()[0]})\n",
        "        # save model\n",
        "        path = Path(wandb.run.dir).joinpath(f\"model{epoch}.pth\")\n",
        "        torch.save(model.state_dict(), path)\n",
        "\n",
        "        # eval\n",
        "        test_accucracy = accuracy(testset=testset, model=model, device=device, batch_size=batch_size)\n",
        "        wandb.log({\"test_accucracy\": test_accucracy})\n",
        "\n",
        "    # save final model\n",
        "    path = Path(wandb.run.dir).joinpath(\"model.pth\")\n",
        "    torch.save(model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### concurrent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_tasks_concurrently(  # noqa: PLR0913\n",
        "    model_dict: dict,\n",
        "    device: torch.device,\n",
        "    epochs: int,\n",
        "    batch_size: int,\n",
        "    lr: float,\n",
        "    weight_decay: float,\n",
        "    criterion: nn.modules.loss._Loss | None = None,\n",
        "    scheduler: torch.optim.lr_scheduler.LRScheduler | None = None,\n",
        ") -> None:\n",
        "    # build model\n",
        "    constructor = model_dict.pop(\"constructor\")\n",
        "    model_name = model_dict.pop(\"name\")\n",
        "    model = constructor(**model_dict)\n",
        "    # create optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    # setup logging\n",
        "    project_name = \"continual_learning\"\n",
        "    run_name = f\"{datetime.now(tz=timezone.utc).strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
        "    config = {\n",
        "        \"training_method\": \"concurrently\",\n",
        "        \"model_name\": model_name,\n",
        "        \"dataset\": \"CIFAR-10\",\n",
        "        \"epochs\": epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"lr\": lr,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"num_parameters\": sum(p.numel() for p in model.parameters()),\n",
        "    }\n",
        "    config.update(model_dict)\n",
        "    wandb.init(\n",
        "        project=project_name,\n",
        "        name=run_name,\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    # get datasets\n",
        "    trainsets, testsets = get_datasets(tasks=1)\n",
        "\n",
        "    train_on_task(\n",
        "        trainset=trainsets[0],\n",
        "        testset=testsets[0],\n",
        "        model=model,\n",
        "        device=device,\n",
        "        optimizer=optimizer,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        lr=lr,\n",
        "        criterion=criterion,\n",
        "        scheduler=scheduler,\n",
        "    )\n",
        "    # finish logging run\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### sequentially\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQIXFn8FGeqR"
      },
      "outputs": [],
      "source": [
        "def train_tasks_sequentially(  # noqa: PLR0913\n",
        "    model_dict: dict,\n",
        "    device: torch.device,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    epochs: int,\n",
        "    batch_size: int,\n",
        "    tasks: int,\n",
        "    lr: float,\n",
        "    weight_decay: float,\n",
        "    criterion: nn.modules.loss._Loss | None = None,\n",
        "    scheduler: torch.optim.lr_scheduler.LRScheduler | None = None,\n",
        ") -> None:\n",
        "    # build model\n",
        "    constructor = model_dict.pop(\"constructor\")\n",
        "    model_name = model_dict.pop(\"name\")\n",
        "    model = constructor(**model_dict)\n",
        "    # create optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    # setup logging\n",
        "    project_name = \"continual_learning\"\n",
        "    run_name = f\"{datetime.now(tz=timezone.utc).strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
        "    config = {\n",
        "        \"training_method\": \"sequentially\",\n",
        "        \"model_name\": model_name,\n",
        "        \"dataset\": \"CIFAR-10\",\n",
        "        \"epochs\": epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"tasks\": tasks,\n",
        "        \"lr\": lr,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"num_parameters\": sum(p.numel() for p in model.parameters()),\n",
        "    }\n",
        "    config.update(model_dict)\n",
        "    wandb.init(\n",
        "        project=project_name,\n",
        "        name=run_name,\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    # get datasets\n",
        "    trainsets, testsets = get_datasets(tasks=tasks)\n",
        "\n",
        "    avg_accs_per_task = []\n",
        "    for k in range(tasks):\n",
        "        # train model on task\n",
        "        train_on_task(\n",
        "            trainset=trainsets[k],\n",
        "            testset=testsets[k],\n",
        "            model=model,\n",
        "            device=device,\n",
        "            optimizer=optimizer,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            criterion=criterion,\n",
        "            scheduler=scheduler,\n",
        "        )\n",
        "        # evaluate model\n",
        "        avg_acc, avg_accs = average_accuracy(\n",
        "            testsets=testsets[: k + 1],  # include current task\n",
        "            model=model,\n",
        "            device=device,\n",
        "            return_intermediate=True,\n",
        "        )\n",
        "        avg_accs_per_task.append(avg_accs)\n",
        "        wandb.log({\"accuracy_on_current_task_only\": avg_accs[-1]})\n",
        "        wandb.log({\"average_accuracy\": avg_acc})\n",
        "\n",
        "        # calculate forgetting measure as defined here https://arxiv.org/pdf/2302.00487.pdf\n",
        "        if k > 0:  # forgetting measure only makes sense, if we already trained on prior task\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"forgetting_measure\": forgetting_measure(\n",
        "                        average_accuracies_per_training_per_task=avg_accs_per_task,\n",
        "                        current_task=k,\n",
        "                    ),\n",
        "                },\n",
        "            )\n",
        "\n",
        "        # save model\n",
        "        path = Path(wandb.run.dir).joinpath(f\"model_task{k}_of{tasks}.pth\")\n",
        "        torch.save(model.state_dict(), path)\n",
        "\n",
        "    # finish logging run\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqsnE1JcGeqS"
      },
      "source": [
        "check accuracy_on_current_task_only\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### rehearsal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_tasks_sequentially_rehearsal(  # noqa: PLR0913\n",
        "    model_dict: dict,\n",
        "    device: torch.device,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    epochs: int,\n",
        "    batch_size: int,\n",
        "    tasks: int,\n",
        "    lr: float,\n",
        "    weight_decay: float,\n",
        "    memory_size_per_task: int,\n",
        "    criterion: nn.modules.loss._Loss | None = None,\n",
        "    scheduler: torch.optim.lr_scheduler.LRScheduler | None = None,\n",
        ") -> None:\n",
        "    # build model\n",
        "    constructor = model_dict.pop(\"constructor\")\n",
        "    model_name = model_dict.pop(\"name\")\n",
        "    model = constructor(**model_dict)\n",
        "    # create optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    # setup logging\n",
        "    project_name = \"continual_learning\"\n",
        "    run_name = f\"{datetime.now(tz=timezone.utc).strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
        "    config = {\n",
        "        \"training_method\": \"sequentially with rehearsal\",\n",
        "        \"model_name\": model_name,\n",
        "        \"dataset\": \"CIFAR-10\",\n",
        "        \"epochs\": epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"tasks\": tasks,\n",
        "        \"lr\": lr,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"memory_size_per_task\": memory_size_per_task,\n",
        "        \"num_parameters\": sum(p.numel() for p in model.parameters()),\n",
        "    }\n",
        "    config.update(model_dict)\n",
        "    wandb.init(\n",
        "        project=project_name,\n",
        "        name=run_name,\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    # get datasets\n",
        "    trainsets, testsets = get_datasets(tasks=tasks)\n",
        "\n",
        "    avg_accs_per_task = []\n",
        "    memories = []\n",
        "    for k in range(tasks):\n",
        "        # train model on task\n",
        "        train_on_task(\n",
        "            trainset=ConcatDataset([trainsets[k], *memories]),\n",
        "            testset=testsets[k],\n",
        "            model=model,\n",
        "            device=device,\n",
        "            optimizer=optimizer,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            criterion=criterion,\n",
        "            scheduler=scheduler,\n",
        "        )\n",
        "        # evaluate model\n",
        "        avg_acc, avg_accs = average_accuracy(\n",
        "            testsets=testsets[: k + 1],  # include current task\n",
        "            model=model,\n",
        "            device=device,\n",
        "            return_intermediate=True,\n",
        "        )\n",
        "        avg_accs_per_task.append(avg_accs)\n",
        "        wandb.log({\"accuracy_on_current_task_only\": avg_accs[-1]})\n",
        "        wandb.log({\"average_accuracy\": avg_acc})\n",
        "\n",
        "        # calculate forgetting measure as defined here https://arxiv.org/pdf/2302.00487.pdf\n",
        "        if k > 0:  # forgetting measure only makes sense, if we already trained on prior task\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"forgetting_measure\": forgetting_measure(\n",
        "                        average_accuracies_per_training_per_task=avg_accs_per_task,\n",
        "                        current_task=k,\n",
        "                    ),\n",
        "                },\n",
        "            )\n",
        "\n",
        "        # save model\n",
        "        path = Path(wandb.run.dir).joinpath(f\"model_task{k}_of{tasks}.pth\")\n",
        "        torch.save(model.state_dict(), path)\n",
        "\n",
        "        # add come images and labels from current task to memory\n",
        "        random_indices = torch.randint(low=0, high=len(trainsets[k - 1]), size=(memory_size_per_task,))\n",
        "        memory_task = Subset(trainsets[k], random_indices)\n",
        "        memories.append(memory_task)\n",
        "\n",
        "    # finish logging run\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### elastic weight consolidation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_on_task_with_elastic_weight_loss(\n",
        "    trainset: Dataset,\n",
        "    testset: Dataset,\n",
        "    model: nn.Module,\n",
        "    device: torch.device,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    epochs: int,\n",
        "    batch_size: int,\n",
        "    lr: float,\n",
        "    fisher_dict: dict,\n",
        "    optpar_dict: dict,\n",
        "    ewc_lambda: float,\n",
        "    current_task: int,\n",
        "    criterion: nn.modules.loss._Loss | None = None,\n",
        "    scheduler: torch.optim.lr_scheduler.LRScheduler | None = None,\n",
        "):\n",
        "    # create dataloaders\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    # move model to device\n",
        "    model.to(device=device)\n",
        "\n",
        "    if criterion is None:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "    if scheduler is None:\n",
        "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer,\n",
        "            max_lr=lr,\n",
        "            steps_per_epoch=len(trainloader),\n",
        "            epochs=epochs,\n",
        "        )\n",
        "\n",
        "    # training\n",
        "    for epoch in range(epochs):\n",
        "        # train one epoch\n",
        "        with tqdm(total=len(trainset), unit=\"images\") as progress_bar:\n",
        "            model.train()\n",
        "            for i, (images, labels) in enumerate(trainloader):\n",
        "                progress_bar.set_description(f\"Epoch {epoch+1} Batch {i}\")\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                prediction = model(images.to(device=device))\n",
        "                # calc loss\n",
        "                loss = criterion(prediction, labels.to(device=device))\n",
        "\n",
        "                # add elastic weight loss\n",
        "                for task in range(current_task):\n",
        "                    for name, param in model.named_parameters():\n",
        "                        fisher = fisher_dict[task][name]\n",
        "                        optpar = optpar_dict[task][name]\n",
        "                        loss += (fisher * (optpar - param).pow(2)).sum() * ewc_lambda\n",
        "\n",
        "                # backward\n",
        "                loss.backward()\n",
        "                # optimizer\n",
        "                optimizer.step()\n",
        "                # scheduler\n",
        "                scheduler.step()\n",
        "                progress_bar.set_postfix(loss=loss.item())\n",
        "                progress_bar.update(labels.shape[0])\n",
        "                wandb.log({\"loss\": loss})\n",
        "                wandb.log({\"lr\": scheduler.get_last_lr()[0]})\n",
        "        # save model\n",
        "        path = Path(wandb.run.dir).joinpath(f\"model{epoch}.pth\")\n",
        "        torch.save(model.state_dict(), path)\n",
        "\n",
        "        # eval\n",
        "        test_accucracy = accuracy(testset=testset, model=model, device=device, batch_size=batch_size)\n",
        "        wandb.log({\"test_accucracy\": test_accucracy})\n",
        "\n",
        "    # save final model\n",
        "    path = Path(wandb.run.dir).joinpath(\"model.pth\")\n",
        "    torch.save(model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_tasks_sequentially_elastic_weight_consolidation(  # noqa: PLR0913\n",
        "    model_dict: dict,\n",
        "    device: torch.device,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    epochs: int,\n",
        "    batch_size: int,\n",
        "    tasks: int,\n",
        "    lr: float,\n",
        "    weight_decay: float,\n",
        "    ewc_lambda: float,\n",
        "    criterion: nn.modules.loss._Loss | None = None,\n",
        "    scheduler: torch.optim.lr_scheduler.LRScheduler | None = None,\n",
        ") -> None:\n",
        "    # build model\n",
        "    constructor = model_dict.pop(\"constructor\")\n",
        "    model_name = model_dict.pop(\"name\")\n",
        "    model = constructor(**model_dict)\n",
        "    # create optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    # setup logging\n",
        "    project_name = \"continual_learning\"\n",
        "    run_name = f\"{datetime.now(tz=timezone.utc).strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
        "    config = {\n",
        "        \"training_method\": \"sequentially with elastic weight consolidation\",\n",
        "        \"model_name\": model_name,\n",
        "        \"dataset\": \"CIFAR-10\",\n",
        "        \"epochs\": epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"tasks\": tasks,\n",
        "        \"lr\": lr,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"ewc_lambda\": ewc_lambda,\n",
        "        \"num_parameters\": sum(p.numel() for p in model.parameters()),\n",
        "    }\n",
        "    config.update(model_dict)\n",
        "    wandb.init(\n",
        "        project=project_name,\n",
        "        name=run_name,\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    # get datasets\n",
        "    trainsets, testsets = get_datasets(tasks=tasks)\n",
        "\n",
        "    avg_accs_per_task = []\n",
        "    fisher_dict = {}\n",
        "    optpar_dict = {}\n",
        "    for k in range(tasks):\n",
        "        train_on_task_with_elastic_weight_loss(\n",
        "            trainset=trainsets[k],\n",
        "            testset=testsets[k],\n",
        "            model=model,\n",
        "            device=device,\n",
        "            optimizer=optimizer,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            fisher_dict=fisher_dict,\n",
        "            optpar_dict=optpar_dict,\n",
        "            ewc_lambda=ewc_lambda,\n",
        "            current_task=k,\n",
        "            criterion=criterion,\n",
        "            scheduler=scheduler,\n",
        "        )\n",
        "        # evaluate model\n",
        "        avg_acc, avg_accs = average_accuracy(\n",
        "            testsets=testsets[: k + 1],  # include current task\n",
        "            model=model,\n",
        "            device=device,\n",
        "            return_intermediate=True,\n",
        "        )\n",
        "        avg_accs_per_task.append(avg_accs)\n",
        "        wandb.log({\"accuracy_on_current_task_only\": avg_accs[-1]})\n",
        "        wandb.log({\"average_accuracy\": avg_acc})\n",
        "\n",
        "        # calculate forgetting measure as defined here https://arxiv.org/pdf/2302.00487.pdf\n",
        "        if k > 0:  # forgetting measure only makes sense, if we already trained on prior task\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"forgetting_measure\": forgetting_measure(\n",
        "                        average_accuracies_per_training_per_task=avg_accs_per_task,\n",
        "                        current_task=k,\n",
        "                    ),\n",
        "                },\n",
        "            )\n",
        "\n",
        "        # save model\n",
        "        path = Path(wandb.run.dir).joinpath(f\"model_task{k}_of{tasks}.pth\")\n",
        "        torch.save(model.state_dict(), path)\n",
        "\n",
        "        # gradients accumulated can be used to calculate fisher\n",
        "        fisher_dict[k] = {}\n",
        "        optpar_dict[k] = {}\n",
        "        for name, param in model.named_parameters():\n",
        "            optpar_dict[k][name] = param.data.clone()\n",
        "            fisher_dict[k][name] = param.grad.data.clone().pow(2)\n",
        "\n",
        "    # finish logging run\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UILvQs6gGeqS"
      },
      "outputs": [],
      "source": [
        "def load_weights_from_wandb(model: nn.Module, run_name: str) -> nn.Module:\n",
        "    best_model = wandb.restore(\n",
        "        \"model.pth\",\n",
        "        run_path=f\"fabianfuchs/continual_learning/{run_name}\",\n",
        "        root=Path.cwd().joinpath(\"checkpoints\"),\n",
        "        replace=True,\n",
        "    )\n",
        "\n",
        "    # use the \"name\" attribute of the returned object if your framework expects a filename, e.g. as in Keras\n",
        "    model.load_state_dict(torch.load(best_model.name))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIt_EXa1GeqS"
      },
      "source": [
        "## Standard Setting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3psLpC6hGeqS"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "batch_size = 64\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "weight_decay = 0.01\n",
        "device = torch.device(\"cuda:0\")\n",
        "tasks = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convnext_minimal = {\n",
        "#     \"constructor\": ConvNeXtV2,\n",
        "#     \"name\": \"ConvNeXtV2\",\n",
        "#     \"in_chans\": 3,\n",
        "#     \"num_classes\": 10,\n",
        "#     \"depths\": [2, 2, 2, 2],\n",
        "#     \"dims\": [128, 128, 128, 128],\n",
        "#     \"patch_size\": 1,\n",
        "# }\n",
        "# models.append(convnext_minimal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmRL4K2LGeqS"
      },
      "outputs": [],
      "source": [
        "# convnext_atto = {\n",
        "#     \"constructor\": ConvNeXtV2,\n",
        "#     \"name\": \"ConvNeXtV2\",\n",
        "#     \"in_chans\": 3,\n",
        "#     \"num_classes\": 10,\n",
        "#     \"depths\": [2, 2, 6, 2],\n",
        "#     \"dims\": [40, 80, 160, 320],\n",
        "#     \"patch_size\": 1,\n",
        "# }\n",
        "# models.append(convnext_atto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX3B6pscGeqT"
      },
      "outputs": [],
      "source": [
        "# convnext_tiny = {\n",
        "#     \"constructor\": ConvNeXtV2,\n",
        "#     \"name\": \"ConvNeXtV2\",\n",
        "#     \"in_chans\": 3,\n",
        "#     \"num_classes\": 10,\n",
        "#     \"depths\": [3, 3, 9, 3],\n",
        "#     \"dims\": [96, 192, 384, 768],\n",
        "#     \"patch_size\": 1,\n",
        "# }\n",
        "# models.append(convnext_tiny)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OZ7CY4EGeqT"
      },
      "outputs": [],
      "source": [
        "# convnext_base = {\n",
        "#     \"constructor\": ConvNeXtV2,\n",
        "#     \"name\": \"ConvNeXtV2\",\n",
        "#     \"in_chans\": 3,\n",
        "#     \"num_classes\": 10,\n",
        "#     \"depths\": [3, 3, 27, 3],\n",
        "#     \"dims\": [128, 256, 512, 1024],\n",
        "#     \"patch_size\": 1,\n",
        "# }\n",
        "# models.append(convnext_base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv_mixer_minimal = {\n",
        "    \"constructor\": ConvMixer,\n",
        "    \"name\": \"ConvMixer\",\n",
        "    \"dim\": 128,\n",
        "    \"depth\": 4,\n",
        "    \"kernel_size\": 7,\n",
        "    \"patch_size\": 1,\n",
        "    \"n_classes\": 10,\n",
        "}\n",
        "models.append(conv_mixer_minimal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# conv_mixer_atto = {\n",
        "#     \"constructor\": ConvMixer,\n",
        "#     \"name\": \"ConvMixer\",\n",
        "#     \"dim\": 128,\n",
        "#     \"depth\": 8,\n",
        "#     \"kernel_size\": 7,\n",
        "#     \"patch_size\": 1,\n",
        "#     \"n_classes\": 10,\n",
        "# }\n",
        "# models.append(conv_mixer_atto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# conv_mixer_tiny = {\n",
        "#     \"constructor\": ConvMixer,\n",
        "#     \"name\": \"ConvMixer\",\n",
        "#     \"dim\": 256,\n",
        "#     \"depth\": 8,\n",
        "#     \"kernel_size\": 7,\n",
        "#     \"patch_size\": 1,\n",
        "#     \"n_classes\": 10,\n",
        "# }\n",
        "# models.append(conv_mixer_tiny)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHEyR98AGeqT"
      },
      "outputs": [],
      "source": [
        "for model in models:\n",
        "    train_tasks_concurrently(\n",
        "        model_dict=model,\n",
        "        device=device,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R59sQDCoGeqT"
      },
      "source": [
        "## Sequential without modifications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwBNBJFzGeqT"
      },
      "outputs": [],
      "source": [
        "tasks = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JqT_GCKNG3T"
      },
      "outputs": [],
      "source": [
        "for model in models:\n",
        "    train_tasks_sequentially(\n",
        "        model=model,\n",
        "        device=device,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        tasks=tasks,\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sequential with rehearsal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4krDh2uGeqT"
      },
      "outputs": [],
      "source": [
        "for model in models:\n",
        "    for memory_size_per_task in [1000, 2000, 5000, 10000]:\n",
        "        train_tasks_sequentially_rehearsal(\n",
        "            model=model,\n",
        "            device=device,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            tasks=tasks,\n",
        "            lr=lr,\n",
        "            weight_decay=weight_decay,\n",
        "            memory_size_per_task=memory_size_per_task,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sequential with elastic weight consolidation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for model in models:\n",
        "    for ewc_lambda in [0.2, 0.4, 0.6]:\n",
        "        train_tasks_sequentially_elastic_weight_consolidation(\n",
        "            model=model,\n",
        "            device=device,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            tasks=tasks,\n",
        "            lr=lr,\n",
        "            weight_decay=weight_decay,\n",
        "            ewc_lambda=ewc_lambda,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
