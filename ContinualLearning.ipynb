{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12675,"status":"ok","timestamp":1712315367126,"user":{"displayName":"Fabian F","userId":"04447055572954868898"},"user_tz":-120},"id":"tIEher0OlQ61","outputId":"36ee0bda-91bf-48aa-e956-a4b9b8963371"},"outputs":[],"source":["#!pip install wandb -qU"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":12220,"status":"ok","timestamp":1712315379343,"user":{"displayName":"Fabian F","userId":"04447055572954868898"},"user_tz":-120},"id":"zJ7xNU3NcCwz"},"outputs":[],"source":["from __future__ import annotations\n","\n","from datetime import datetime, timezone\n","from pathlib import Path\n","\n","import torch\n","import torch.nn.functional as F\n","import torchvision\n","from torch import nn, optim\n","from torch.utils.data import DataLoader, Dataset, Subset\n","from torchvision import transforms\n","from tqdm import tqdm\n","\n","import wandb"]},{"cell_type":"markdown","metadata":{"id":"zk95CyQP6Sfa"},"source":["## Weights and Bias Login\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"elapsed":26879,"status":"ok","timestamp":1712315406219,"user":{"displayName":"Fabian F","userId":"04447055572954868898"},"user_tz":-120},"id":"Amah_vX1lNE-","outputId":"a5248a1e-9738-463e-c574-1eb65ccc9b9c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","wandb: Currently logged in as: fabianfuchs. Use `wandb login --relogin` to force relogin\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login()"]},{"cell_type":"markdown","metadata":{"id":"7odmCw657Me2"},"source":["## Model\n"]},{"cell_type":"markdown","metadata":{},"source":["Got source code for the ConvNeXtV2 model from https://github.com/facebookresearch/ConvNeXt-V2/blob/main/models/convnextv2.py and removed drop path and custom weight initialization. Added variable patch size.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":209,"status":"ok","timestamp":1712315409552,"user":{"displayName":"Fabian F","userId":"04447055572954868898"},"user_tz":-120},"id":"kA6kljA77MAe"},"outputs":[],"source":["class LayerNorm(nn.Module):\n","    \"\"\"LayerNorm that supports two data formats: channels_last (default) or channels_first.\n","    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n","    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n","    with shape (batch_size, channels, height, width).\n","    \"\"\"\n","\n","    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(normalized_shape))\n","        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n","        self.eps = eps\n","        self.data_format = data_format\n","        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n","            raise NotImplementedError\n","        self.normalized_shape = (normalized_shape,)\n","\n","    def forward(self, x):\n","        if self.data_format == \"channels_last\":\n","            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n","        elif self.data_format == \"channels_first\":\n","            u = x.mean(1, keepdim=True)\n","            s = (x - u).pow(2).mean(1, keepdim=True)\n","            x = (x - u) / torch.sqrt(s + self.eps)\n","            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n","            return x\n","\n","\n","class GRN(nn.Module):\n","    \"\"\"GRN (Global Response Normalization) layer\"\"\"\n","\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.gamma = nn.Parameter(torch.zeros(1, 1, 1, dim))\n","        self.beta = nn.Parameter(torch.zeros(1, 1, 1, dim))\n","\n","    def forward(self, x):\n","        Gx = torch.norm(x, p=2, dim=(1, 2), keepdim=True)\n","        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n","        return self.gamma * (x * Nx) + self.beta + x\n","\n","\n","class Block(nn.Module):\n","    \"\"\"ConvNeXtV2 Block.\"\"\"\n","\n","    def __init__(self, dim, drop_path=0.0):\n","        \"\"\"ConvNeXtV2 Block.\n","\n","        Args:\n","            dim (int): Number of input channels.\n","            drop_path (float): Stochastic depth rate. Default: 0.0\n","        \"\"\"\n","        super().__init__()\n","        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)  # depthwise conv\n","        self.norm = LayerNorm(dim, eps=1e-6)\n","        self.pwconv1 = nn.Linear(dim, 4 * dim)  # pointwise/1x1 convs, implemented with linear layers\n","        self.act = nn.GELU()\n","        self.grn = GRN(4 * dim)\n","        self.pwconv2 = nn.Linear(4 * dim, dim)\n","\n","    def forward(self, x):\n","        input = x\n","        x = self.dwconv(x)\n","        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n","        x = self.norm(x)\n","        x = self.pwconv1(x)\n","        x = self.act(x)\n","        x = self.grn(x)\n","        x = self.pwconv2(x)\n","        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n","\n","        return input + x\n","\n","\n","class ConvNeXtV2(nn.Module):\n","    \"\"\"ConvNeXt V2.\"\"\"\n","\n","    def __init__(\n","        self,\n","        in_chans=3,\n","        num_classes=1000,\n","        depths=[3, 3, 9, 3],\n","        dims=[96, 192, 384, 768],\n","        drop_path_rate=0.0,\n","        patch_size=1,\n","    ):\n","        \"\"\"ConvNeXt V2.\n","\n","        Args:\n","            in_chans (int): Number of input image channels. Default: 3\n","            num_classes (int): Number of classes for classification head. Default: 1000\n","            depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n","            dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n","            drop_path_rate (float): Stochastic depth rate. Default: 0.\n","            head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n","        \"\"\"\n","        super().__init__()\n","        self.depths = depths\n","        self.downsample_layers = nn.ModuleList()  # stem and 3 intermediate downsampling conv layers\n","        stem = nn.Sequential(\n","            nn.Conv2d(in_chans, dims[0], kernel_size=patch_size, stride=patch_size),\n","            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\"),\n","        )\n","        self.downsample_layers.append(stem)\n","        for i in range(3):\n","            downsample_layer = nn.Sequential(\n","                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n","                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2),\n","            )\n","            self.downsample_layers.append(downsample_layer)\n","\n","        self.stages = nn.ModuleList()  # 4 feature resolution stages, each consisting of multiple residual blocks\n","        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n","        cur = 0\n","        for i in range(4):\n","            stage = nn.Sequential(*[Block(dim=dims[i], drop_path=dp_rates[cur + j]) for j in range(depths[i])])\n","            self.stages.append(stage)\n","            cur += depths[i]\n","\n","        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # final norm layer\n","        self.head = nn.Linear(dims[-1], num_classes)\n","\n","    def forward_features(self, x):\n","        for i in range(4):\n","            x = self.downsample_layers[i](x)\n","            x = self.stages[i](x)\n","        return self.norm(x.mean([-2, -1]))  # global average pooling, (N, C, H, W) -> (N, C)\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        x = self.head(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"gS1TSvMr6VrH"},"source":["## Functions\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":215,"status":"ok","timestamp":1712315412651,"user":{"displayName":"Fabian F","userId":"04447055572954868898"},"user_tz":-120},"id":"QNKuM6tj6Q7q"},"outputs":[],"source":["def get_classes() -> tuple:\n","    \"\"\"Return class labels of CIFAR-10 dataset.\"\"\"\n","    return (\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n","\n","\n","def get_datasets(tasks: int) -> tuple[list[Subset], list[Subset]]:\n","    \"\"\"Split CIFAR-10 dataset into task specific subsets.\n","\n","    Args:\n","        tasks (int): Number of tasks to split the dataset into.\n","\n","    Returns:\n","        tuple[list[Subset], list[Subset]]: Tuple containing two list with the train and test subsets.\n","    \"\"\"\n","    classes = get_classes()\n","    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","    trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n","    testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n","    classes_per_task = torch.linspace(0, len(classes), tasks + 1, dtype=torch.int)\n","    trainsets = []\n","    testsets = []\n","    train_targets = torch.tensor(trainset.targets)\n","    test_targets = torch.tensor(testset.targets)\n","    for i in range(len(classes_per_task) - 1):\n","        train_indices = []\n","        test_indices = []\n","        for j in range(classes_per_task[i], classes_per_task[i + 1]):\n","            train_indices.extend((train_targets == j).nonzero(as_tuple=False).flatten().tolist())\n","            test_indices.extend((test_targets == j).nonzero(as_tuple=False).flatten().tolist())\n","        trainsets.append(Subset(trainset, train_indices))\n","        testsets.append(Subset(testset, test_indices))\n","    return trainsets, testsets"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def accuracy(testset: Dataset, model: nn.Module, device: torch.device) -> float:\n","    testloader = DataLoader(testset, shuffle=False)\n","\n","    model.eval()\n","    correct = 0\n","    for images, labels in testloader:\n","        # calculate outputs by running images through the network\n","        predictions = model(images.to(device=device))\n","        # the class with the highest energy is what we choose as prediction\n","        _, predicted = torch.max(predictions.data, 1)\n","        correct += (predicted == labels.to(device=device)).sum().item()\n","    return 100 * correct / len(testloader)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1712315443516,"user":{"displayName":"Fabian F","userId":"04447055572954868898"},"user_tz":-120},"id":"IbR9W9Tw7AVI"},"outputs":[],"source":["def train_on_task(\n","    trainset: Dataset,\n","    testset: Dataset,\n","    model: nn.Module,\n","    device: torch.device,\n","    optimizer: torch.optim.Optimizer,\n","    epochs: int,\n","    batch_size: int,\n","    lr: float,\n","    criterion: nn.modules.loss._Loss | None = None,\n","    scheduler: torch.optim.lr_scheduler.LRScheduler | None = None,\n","):\n","    # create dataloaders\n","    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","    # move model to device\n","    model.to(device=device)\n","\n","    if criterion is None:\n","        criterion = nn.CrossEntropyLoss()\n","    if scheduler is None:\n","        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n","            optimizer,\n","            max_lr=lr,\n","            steps_per_epoch=len(trainloader),\n","            epochs=epochs,\n","        )\n","\n","    # training\n","    for epoch in range(epochs):\n","        # train one epoch\n","        with tqdm(total=len(trainset), unit=\"images\") as progress_bar:\n","            model.train()\n","            for i, (images, labels) in enumerate(trainloader):\n","                progress_bar.set_description(f\"Epoch {epoch+1} Batch {i}\")\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                prediction = model(images.to(device=device))\n","                # calc loss\n","                loss = criterion(prediction, labels.to(device=device))\n","                # backward\n","                loss.backward()\n","                # optimizer\n","                optimizer.step()\n","                # scheduler\n","                scheduler.step()\n","                progress_bar.set_postfix(loss=loss.item())\n","                progress_bar.update(labels.shape[0])\n","                wandb.log({\"loss\": loss})\n","                wandb.log({\"lr\": scheduler.get_last_lr()[0]})\n","        # save model\n","        path = Path(wandb.run.dir).joinpath(f\"model{epoch}.pth\")\n","        torch.save(model.state_dict(), path)\n","\n","        # eval\n","        test_accucracy = accuracy(testset=testset, model=model, device=device)\n","        wandb.log({\"test_accucracy\": test_accucracy})\n","\n","    # save final model\n","    path = Path(wandb.run.dir).joinpath(\"model.pth\")\n","    torch.save(model.state_dict(), path)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def average_accuracy(\n","    testsets: list[Dataset],\n","    model: nn.Module,\n","    device: torch.device,\n","    return_intermediate: bool = False,  # noqa: FBT001, FBT002\n",") -> float | tuple[float, list[float]]:\n","    average_accuracy = 0\n","    average_accuracies = []\n","    for i in range(len(testsets)):\n","        task_accuracy = accuracy(testset=testsets[i], model=model, device=device)\n","        average_accuracy += task_accuracy\n","        average_accuracies.append(task_accuracy)\n","    if return_intermediate:\n","        return average_accuracy / len(testsets), average_accuracies\n","    return average_accuracy / len(testsets)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def train_tasks_sequentially(  # noqa: PLR0913\n","    model: nn.Module,\n","    device: torch.device,\n","    optimizer: torch.optim.Optimizer,\n","    epochs: int,\n","    batch_size: int,\n","    tasks: int,\n","    lr: float,\n","    criterion: nn.modules.loss._Loss | None = None,\n","    scheduler: torch.optim.lr_scheduler.LRScheduler | None = None,\n",") -> None:\n","    # setup logging\n","    project_name = \"continual_learning\"\n","    run_name = f\"{datetime.now(tz=timezone.utc).strftime('%Y_%m_%d_%H_%M_%S')}\"\n","    wandb.init(\n","        project=project_name,\n","        name=run_name,\n","        config={\n","            \"architecture\": \"CNN\",\n","            \"dataset\": \"CIFAR-10\",\n","            \"epochs\": epochs,\n","            \"batch_size\": batch_size,\n","            \"tasks\": tasks,\n","            \"lr\": lr,\n","        },\n","    )\n","\n","    # get datasets\n","    trainsets, testsets = get_datasets(tasks=tasks)\n","\n","    avg_accs_per_task = []\n","    for k in range(tasks):\n","        # train model on task\n","        train_on_task(\n","            trainset=trainsets[k],\n","            testset=testsets[k],\n","            model=model,\n","            device=device,\n","            optimizer=optimizer,\n","            epochs=epochs,\n","            batch_size=batch_size,\n","            lr=lr,\n","            criterion=criterion,\n","            scheduler=scheduler,\n","        )\n","        # evaluate model\n","        avg_acc, avg_accs = average_accuracy(\n","            testsets=testsets[: k + 1],  # include current task\n","            model=model,\n","            device=device,\n","            return_intermediate=True,\n","        )\n","        avg_accs_per_task.append(avg_accs)\n","        wandb.log({\"accuracy_on_current_task_only\": avg_accs[-1]})\n","        wandb.log({\"average_accuracy\": avg_acc})\n","\n","        # calculate forgetting measure as defined here https://arxiv.org/pdf/2302.00487.pdf\n","        if k > 0:  # forgetting measure only makes sense, if we already trained on prior task\n","            forgetting_measure = 0\n","            for j in range(k):  # exclude current task\n","                f = 0\n","                for i in range(k):  # exclude current task\n","                    try:\n","                        f_ = avg_accs_per_task[i][j] - avg_accs_per_task[k][j]\n","                    except IndexError:\n","                        continue\n","                    if f_ > f:\n","                        f = f_\n","                forgetting_measure += f\n","            forgetting_measure /= k\n","            wandb.log({\"forgetting_measure\": forgetting_measure})\n","\n","    # finish logging run\n","    wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["swap loop order?\n"]},{"cell_type":"markdown","metadata":{},"source":["check accuracy_on_current_task_only\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def load_weights_from_wandb(model: nn.Module, run_name: str) -> nn.Module:\n","    best_model = wandb.restore(\n","        \"model.pth\",\n","        run_path=f\"fabianfuchs/continual_learning/{run_name}\",\n","        root=Path.cwd().joinpath(\"checkpoints\"),\n","        replace=True,\n","    )\n","\n","    # use the \"name\" attribute of the returned object if your framework expects a filename, e.g. as in Keras\n","    model.load_state_dict(torch.load(best_model.name))\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["## Standard Setting\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["epochs = 1\n","batch_size = 128\n","lr = 0.01\n","momentum = 0.9\n","device = torch.device(\"cpu\")\n","tasks = 1"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["convnext_atto = ConvNeXtV2(in_chans=3, num_classes=10, depths=[2, 2, 6, 2], dims=[40, 80, 160, 320], patch_size=1)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["convnext_tiny = ConvNeXtV2(in_chans=3, num_classes=10, depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], patch_size=1)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["convnext_base = ConvNeXtV2(in_chans=3, num_classes=10, depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], patch_size=1)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/html":["wandb version 0.16.6 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>c:\\Users\\fabia\\Desktop\\ContinualLearning\\wandb\\run-20240406_080954-8pq1ukmt</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/fabianfuchs/continual_learning/runs/8pq1ukmt/workspace' target=\"_blank\">2024_04_06_06_09_54</a></strong> to <a href='https://wandb.ai/fabianfuchs/continual_learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/fabianfuchs/continual_learning' target=\"_blank\">https://wandb.ai/fabianfuchs/continual_learning</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/fabianfuchs/continual_learning/runs/8pq1ukmt/workspace' target=\"_blank\">https://wandb.ai/fabianfuchs/continual_learning/runs/8pq1ukmt/workspace</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 Batch 206:  53%|█████▎    | 26368/50000 [18:47<17:59, 21.90images/s, loss=1.64]"]}],"source":["for convnext in [convnext_atto, convnext_tiny, convnext_base]:\n","    optimizer = optim.SGD(convnext.parameters(), lr=lr, momentum=momentum)\n","    train_tasks_sequentially(\n","        model=convnext,\n","        device=device,\n","        optimizer=optimizer,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        tasks=tasks,\n","        lr=lr,\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["## Sequential without modifications\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tasks = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1712302779941,"user":{"displayName":"Fabian F","userId":"04447055572954868898"},"user_tz":-120},"id":"0JqT_GCKNG3T"},"outputs":[],"source":["for convnext in [convnext_atto, convnext_tiny, convnext_base]:\n","    optimizer = optim.SGD(convnext.parameters(), lr=lr, momentum=momentum)\n","    train_tasks_sequentially(\n","        model=convnext,\n","        device=device,\n","        optimizer=optimizer,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        tasks=tasks,\n","        lr=lr,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOQvTgp7nDoJbAvqKe/0cP4","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
